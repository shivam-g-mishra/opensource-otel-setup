# =============================================================================
# OpenTelemetry Stack Alerting Rules
# =============================================================================
# Alerts for monitoring the observability stack itself.
# Import additional community alerts as needed.
# =============================================================================

groups:
  # ===========================================================================
  # OTel Collector Alerts
  # ===========================================================================
  - name: otel-collector
    rules:
      - alert: OTelCollectorDown
        expr: up{job="otel-collector"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "OTel Collector is down"
          description: "The OpenTelemetry Collector has been down for more than 1 minute."

      - alert: OTelCollectorHighMemory
        expr: otelcol_process_memory_rss / 1024 / 1024 > 1600
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "OTel Collector high memory usage"
          description: "OTel Collector memory usage is above 1.6GB ({{ $value | printf \"%.0f\" }}MB)."

      - alert: OTelCollectorQueueFilling
        expr: otelcol_exporter_queue_size > 5000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "OTel Collector queue filling up"
          description: "Export queue has {{ $value }} items (>5000). Backend may be slow or down."

      - alert: OTelCollectorQueueFull
        expr: otelcol_exporter_queue_size > 9000
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "OTel Collector queue nearly full"
          description: "Export queue has {{ $value }} items (>9000). Data loss imminent."

      - alert: OTelCollectorDroppedSpans
        expr: rate(otelcol_processor_dropped_spans[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "OTel Collector dropping spans"
          description: "Collector is dropping {{ $value | printf \"%.2f\" }} spans/sec."

      - alert: OTelCollectorDroppedMetrics
        expr: rate(otelcol_processor_dropped_metric_points[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "OTel Collector dropping metrics"
          description: "Collector is dropping {{ $value | printf \"%.2f\" }} metric points/sec."

      - alert: OTelCollectorDroppedLogs
        expr: rate(otelcol_processor_dropped_log_records[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "OTel Collector dropping logs"
          description: "Collector is dropping {{ $value | printf \"%.2f\" }} log records/sec."

      - alert: OTelCollectorExportFailures
        expr: rate(otelcol_exporter_send_failed_spans[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "OTel Collector export failures"
          description: "Collector failing to export {{ $value | printf \"%.2f\" }} spans/sec."

      - alert: OTelCollectorHighCPU
        expr: rate(otelcol_process_cpu_seconds[5m]) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "OTel Collector high CPU usage"
          description: "Collector CPU usage is {{ $value | printf \"%.1f\" }}%."

  # ===========================================================================
  # Prometheus Alerts
  # ===========================================================================
  - name: prometheus
    rules:
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus is down"
          description: "Prometheus has been down for more than 1 minute."

      - alert: PrometheusHighMemory
        expr: process_resident_memory_bytes{job="prometheus"} / 1024 / 1024 / 1024 > 3
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus high memory usage"
          description: "Prometheus memory usage is above 3GB ({{ $value | printf \"%.1f\" }}GB)."

      - alert: PrometheusStorageFilling
        expr: (prometheus_tsdb_storage_blocks_bytes / 1024 / 1024 / 1024) > 40
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus storage filling up"
          description: "Prometheus storage is {{ $value | printf \"%.1f\" }}GB."

      - alert: PrometheusTargetsMissing
        expr: up == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus target down"
          description: "Target {{ $labels.job }} ({{ $labels.instance }}) is down."

      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus config reload failed"
          description: "Prometheus failed to reload configuration."

  # ===========================================================================
  # Loki Alerts
  # ===========================================================================
  - name: loki
    rules:
      - alert: LokiDown
        expr: up{job="loki"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Loki is down"
          description: "Loki has been down for more than 1 minute."

      - alert: LokiIngestionErrors
        expr: rate(loki_distributor_ingester_append_failures_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Loki ingestion failures"
          description: "Loki is experiencing ingestion failures."

      - alert: LokiRequestErrors
        expr: rate(loki_request_duration_seconds_count{status_code=~"5.."}[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Loki request errors"
          description: "Loki is returning 5xx errors."

  # ===========================================================================
  # Jaeger Alerts
  # ===========================================================================
  - name: jaeger
    rules:
      - alert: JaegerDown
        expr: up{job="jaeger"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Jaeger is down"
          description: "Jaeger has been down for more than 1 minute."

      - alert: JaegerStorageHigh
        expr: jaeger_badger_lsm_size_bytes / 1024 / 1024 / 1024 > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Jaeger storage high"
          description: "Jaeger Badger storage is {{ $value | printf \"%.1f\" }}GB."

  # ===========================================================================
  # Grafana Alerts
  # ===========================================================================
  - name: grafana
    rules:
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Grafana is down"
          description: "Grafana has been down for more than 1 minute."

  # ===========================================================================
  # Node Exporter / Infrastructure Alerts
  # ===========================================================================
  - name: node-exporter
    rules:
      - alert: NodeExporterDown
        expr: up{job="node-exporter"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Node Exporter is down"
          description: "Node Exporter has been down for more than 1 minute. Host metrics unavailable."

      - alert: HighDiskUsage
        expr: 100 - ((node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay"}) * 100) > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage on {{ $labels.mountpoint }}"
          description: "Disk usage on {{ $labels.mountpoint }} is {{ $value | printf \"%.1f\" }}% (>85%)."

      - alert: CriticalDiskUsage
        expr: 100 - ((node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay"}) * 100) > 95
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk usage on {{ $labels.mountpoint }}"
          description: "Disk usage on {{ $labels.mountpoint }} is {{ $value | printf \"%.1f\" }}% (>95%). Immediate action required."

      - alert: DiskWillFillIn24Hours
        expr: predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs|overlay"}[6h], 24*60*60) < 0
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Disk {{ $labels.mountpoint }} will fill within 24 hours"
          description: "Based on current trend, disk {{ $labels.mountpoint }} will be full in less than 24 hours."

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | printf \"%.1f\" }}% (>85%)."

      - alert: CriticalMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical memory usage"
          description: "Memory usage is {{ $value | printf \"%.1f\" }}% (>95%). OOM risk."

      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value | printf \"%.1f\" }}% (>85%) for more than 10 minutes."

      - alert: HighLoadAverage
        expr: node_load15 / count without(cpu, mode) (node_cpu_seconds_total{mode="idle"}) > 2
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "High load average"
          description: "15-minute load average is {{ $value | printf \"%.2f\" }} per CPU core."

      - alert: HighNetworkTraffic
        expr: rate(node_network_receive_bytes_total{device!~"lo|veth.*|docker.*|br-.*"}[5m]) / 1024 / 1024 > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High network traffic on {{ $labels.device }}"
          description: "Network receive rate on {{ $labels.device }} is {{ $value | printf \"%.1f\" }} MB/s."

      - alert: NetworkInterfaceDown
        expr: node_network_up{device!~"lo|veth.*|docker.*|br-.*"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Network interface {{ $labels.device }} is down"
          description: "Network interface {{ $labels.device }} has been down for more than 2 minutes."

      - alert: HighInodeUsage
        expr: 100 - ((node_filesystem_files_free{fstype!~"tmpfs|overlay"} / node_filesystem_files{fstype!~"tmpfs|overlay"}) * 100) > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High inode usage on {{ $labels.mountpoint }}"
          description: "Inode usage on {{ $labels.mountpoint }} is {{ $value | printf \"%.1f\" }}% (>85%)."

      - alert: SystemdServiceFailed
        expr: node_systemd_unit_state{state="failed"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Systemd service {{ $labels.name }} failed"
          description: "Systemd service {{ $labels.name }} is in failed state."

      - alert: ClockSkew
        expr: abs(node_timex_offset_seconds) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Clock skew detected"
          description: "Clock skew is {{ $value | printf \"%.3f\" }} seconds. This may cause issues with distributed systems."
